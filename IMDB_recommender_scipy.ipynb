{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as FN\n",
    "#import stanfordnlp\n",
    "import tqdm\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import spotlight\n",
    "import pickle\n",
    "import time \n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "\n",
    "def csv_to_sparse(df_transactions):\n",
    "\n",
    "    df_transactions = df_transactions.drop_duplicates()\n",
    "\n",
    "    users=list(df_transactions[\"customer_id\"])\n",
    "    \n",
    "    items=list(df_transactions[\"item_id\"])\n",
    "    \n",
    "    unique_users=list(set(df_transactions[\"customer_id\"]))\n",
    "    print(len(unique_users))\n",
    "    \n",
    "    unique_items=list(set(df_transactions[\"item_id\"]))\n",
    "    \n",
    "    map_users={}\n",
    "    \n",
    "    map_val=0\n",
    "    \n",
    "    user_list=[]\n",
    "    \n",
    "    for user in unique_users:\n",
    "        map_users[user]=map_val\n",
    "        map_val+=1\n",
    "    \n",
    "    map_items={}\n",
    "    \n",
    "    item_list=[]\n",
    "    \n",
    "    for item in unique_items:\n",
    "        map_items[item]=map_val\n",
    "        map_val+=1\n",
    "    \n",
    "    for val in range(len(users)):\n",
    "        user_list.append(map_users[users[val]])\n",
    "        item_list.append(map_items[items[val]])\n",
    "    \n",
    "\n",
    "    rows=user_list\n",
    "\n",
    "    cols=item_list\n",
    "    \n",
    "    ratings=list(df_transactions[\"price\"])\n",
    "    \n",
    "    matrix = ss.coo_matrix((ratings, (rows, cols)))\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataSource:\n",
    "    CASSANDRA = \"cassandra\"\n",
    "    S3 = \"s3\"\n",
    "    JSON = \"json\"\n",
    "    LOCAL = \"local\"\n",
    "\n",
    "   \n",
    "# DO NOT EDIT\n",
    "# Exception Classes\n",
    "\n",
    "class PZAIException(Exception):\n",
    "    \"\"\"\n",
    "    Base class for other exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DataSourceNotDefinedError(PZAIException):\n",
    "    \"\"\"\n",
    "    Raised when a particular data source is not defined for data import or export.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class PortInformationNotFoundError(PZAIException):\n",
    "    \"\"\"\n",
    "    Raised when port details are not found for the particular port.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class UnknownOperationMode(PZAIException):\n",
    "    \"\"\"\n",
    "    Raised when a method or feature is not defined.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# DO NOT EDIT\n",
    "# Connection Manager Class\n",
    "\n",
    "class ConnectionManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_for_a_port(data, port_number, connection_type):\n",
    "        \"\"\"\n",
    "        This method helps to get the details for one particular port information\n",
    "        :param data:\n",
    "        :param port_number:\n",
    "        :param connection_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for each_data in data:\n",
    "            if int(each_data[\"port\"]) == int(port_number):\n",
    "                return each_data[\"sourceDetails\"]\n",
    "        raise PortInformationNotFoundError(\n",
    "            \"Port details not found for port_number {} in the {} connection\".format(port_number, connection_type))\n",
    "       \n",
    "    \n",
    "    \n",
    "def create_cassandra_table_if_not_exists(df, keypace, table_name, pk):\n",
    "    cassandra_keyspace = keypace\n",
    "    cassandra_table = table_name\n",
    "    schema_string = get_schema_string(df, pk)\n",
    "\n",
    "    query_cassandra_create_table = \"\"\"\n",
    "    CREATE TABLE {keyspace}.{table}({schema_string});\n",
    "    \"\"\".format(keyspace=cassandra_keyspace, table=cassandra_table, schema_string=schema_string)\n",
    "\n",
    "    print(query_cassandra_create_table)\n",
    "    drop_query = \"DROP TABLE IF EXISTS {keyspace}.{table}\".format(keyspace=cassandra_keyspace,table=cassandra_table)\n",
    "    session.execute(drop_query)\n",
    "    session.execute(query_cassandra_create_table)\n",
    "\n",
    "def get_schema_string(df, primary_k):\n",
    "    datatype_changer_dict = {\n",
    "        \"StringType\": \"text\",\n",
    "        \"IntegerType\": \"int\",\n",
    "        \"DateType\": \"int\",\n",
    "        \"LongType\": \"int\",\n",
    "        \"DoubleType\": \"double\"\n",
    "    }\n",
    "    columns = df.limit(2).toPandas().columns\n",
    "    schema_string = []\n",
    "    for each_column in columns:\n",
    "        data_type = str(list(df[[each_column]].schema)[0]).split(\",\")[1]\n",
    "        try:\n",
    "            data_type = datatype_changer_dict[data_type]\n",
    "        except:\n",
    "            data_type = None\n",
    "        schema_string.append(\"{column_name} {data_type}\".format(column_name=each_column, data_type=data_type))\n",
    "    schema_string.append(\"PRIMARY KEY ({})\".format(primary_k))\n",
    "    schema_string = \",\".join(schema_string)\n",
    "    return schema_string\n",
    "# DO NOT EDIT\n",
    "# Dataframe Connector\n",
    "\n",
    "class DataFrame(object):\n",
    "    \"\"\"\n",
    "    class that gets a data-frame from the mentioned port of the input-data\n",
    "    \"\"\"\n",
    "\n",
    "    def get(self, input_data, port_number):\n",
    "        \"\"\"\n",
    "        The function that gets the entire input configuration for the data and returns the selected data-frame.\n",
    "        :param input_data: list of dictionary for input configuration.\n",
    "        :param port_number: the port number from where the data has to be fetched.\n",
    "        :return: spark data-frame\n",
    "        \"\"\"\n",
    "        return self._get_df(input_data=input_data, port_number=port_number)\n",
    "\n",
    "    def _get_df(self, input_data, port_number):\n",
    "        \"\"\"\n",
    "        The function that gets the entire input configuration for the data and returns the selected data-frame.\n",
    "        :param input_data: list od dictionary for input configuration.\n",
    "        :param port_number: the port number from where the data has to be fetched.\n",
    "        :return: spark data-frame\n",
    "        \"\"\"\n",
    "        port_information = ConnectionManager.get_data_for_a_port(data=input_data,\n",
    "                                                                 port_number=port_number,\n",
    "                                                                 connection_type=\"input\")\n",
    "        data_source = str(port_information[\"source\"]).lower()\n",
    "        if data_source == DataSource.CASSANDRA:\n",
    "            df = self._get_df_from_cassandra(port_information)\n",
    "        elif data_source == DataSource.S3:\n",
    "            df = self._get_df_from_s3(port_information)\n",
    "        elif data_source==DataSource.LOCAL:\n",
    "            df=self._get_df_from_local(port_information)\n",
    "        else:\n",
    "            raise DataSourceNotDefinedError(\"Data-frame import from {} is currently not supported.\".format(data_source))\n",
    "        return df\n",
    "\n",
    "    def write(self, df, output_data, port_number):\n",
    "        \"\"\"\n",
    "        The function that gets the entire input configuration for the data and returns the selected data-frame.\n",
    "        :param df: The spark data-frame to be written out.\n",
    "        :param output_data: list of dictionary for output configuration.\n",
    "        :param port_number: the port number to which the data has to be written.\n",
    "        :return: boolean status\n",
    "        \"\"\"\n",
    "        return self.__write(df=df, output_data=output_data, port_number=port_number)\n",
    "\n",
    "    def __write(self, df, output_data, port_number):\n",
    "        \"\"\"\n",
    "        The function that gets the entire input configuration for the data and returns the selected data-frame.\n",
    "        :param df: The spark data-frame to be written out.\n",
    "        :param output_data: list of dictionary for output configuration.\n",
    "        :param port_number: the port number to which the data has to be written.\n",
    "        :return: boolean status\n",
    "        \"\"\"\n",
    "        port_information = ConnectionManager.get_data_for_a_port(data=output_data,\n",
    "                                                                 port_number=port_number,\n",
    "                                                                 connection_type=\"output\")\n",
    "        data_source = str(port_information[\"source\"]).lower()\n",
    "\n",
    "        if data_source == DataSource.CASSANDRA:\n",
    "            flag = self.__write_to_cassandra(df=df, source_information=port_information)\n",
    "        elif data_source == DataSource.S3:\n",
    "            flag = self.__write_to_s3(df=df, source_information=port_information)\n",
    "        else:\n",
    "            raise DataSourceNotDefinedError(\"Data-frame export to {} is currently not supported.\".format(data_source))\n",
    "        return flag\n",
    "\n",
    "    @staticmethod\n",
    "    def __write_to_cassandra(df, source_information):\n",
    "        \"\"\"\n",
    "        The function to write data to a cassandra table.\n",
    "        :param df:\n",
    "        :param source_information:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        table_name = source_information[\"tableName\"]\n",
    "        keyspace_name = source_information[\"keyspace\"]\n",
    "        write_mode = str(source_information[\"writeMode\"]).lower()\n",
    "        pk = str(source_information[\"primaryKeys\"]).lower()\n",
    "\n",
    "        for col in df.columns:\n",
    "            df = df.withColumnRenamed(col, col.lower())\n",
    "\n",
    "        if write_mode == \"append\":\n",
    "            df.write.format(\"org.apache.spark.sql.cassandra\").mode(write_mode).options(table=table_name,\n",
    "                                                                                       keyspace=keyspace_name).save()\n",
    "        elif write_mode == \"overwrite\":\n",
    "            create_cassandra_table_if_not_exists(df, keyspace_name, table_name, pk)\n",
    "            df.write.format(\"org.apache.spark.sql.cassandra\").mode(\"overwrite\").options(table=table_name,keyspace=keyspace_name).option(\"confirm.truncate\", \"true\").save()\n",
    "\n",
    "        else:\n",
    "            raise UnknownOperationMode(\"The mentioned writing mode {} is not defined for Cassandra.\".format(write_mode))\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def __write_to_s3(df, source_information):\n",
    "        file_path = source_information[\"filePath\"]\n",
    "        file_format = source_information[\"fileFormat\"]\n",
    "        df.write.format(file_format).options(header='true').mode(\"overwrite\").save(file_path)        \n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_df_from_cassandra(source_information):\n",
    "        \"\"\"\n",
    "        The function to get data-frame from a cassandra table.\n",
    "        :param source_information:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        table_name = source_information[\"tableName\"]\n",
    "        keyspace_name = source_information[\"keyspace\"]\n",
    "        df = spark.read.format('org.apache.spark.sql.cassandra').options(table=table_name,\n",
    "                                                                         keyspace=keyspace_name).load()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_df_from_s3(source_information):\n",
    "        \"\"\"\n",
    "        The function to get data-frame from Amazon S3.\n",
    "        :param source_information:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        file_format = source_information[\"fileFormat\"]\n",
    "        file_path = source_information[\"filePath\"]\n",
    "        df = spark.read.format(file_format).options(header='true', inferSchema='true').load(file_path)\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def _get_df_from_local(source_information):\n",
    "\n",
    "        df=pd.DataFrame()\n",
    "        file_format = source_information[\"fileFormat\"]\n",
    "        file_path = source_information[\"filePath\"]\n",
    "        try:\n",
    "            for filename in os.listdir(file_path):\n",
    "                if filename.endswith(file_format):\n",
    "                    if file_format==\"csv\":\n",
    "                        df1=pd.read_csv(file_path+\"/\"+filename)\n",
    "                        df=pd.concat([df,df1])\n",
    "                    elif file_format==\"parquet\":\n",
    "                        df1=pd.read_parquet(file_path+\"/\"+filename,engine=\"pyarrow\")\n",
    "                        df=pd.concat([df,df1])\n",
    "        except:\n",
    "            if file_format==\"csv\":\n",
    "                df=pd.read_csv(file_path)\n",
    "\n",
    "            elif file_format==\"parquet\":\n",
    "                df=pd.read_parquet(file_path,engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "request_data ={\n",
    "  \"input\": [\n",
    "    {\n",
    "      \"port\": 1,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"local\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\":r\"C:\\Users\\pv23228\\Documents\\P.AI\\Data\\Movies Dataset\\final_transactions_ss_trainset_v1.csv\"\n",
    "    }\n",
    "    },\n",
    "      {\n",
    "      \"port\": 2,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"local\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\": r\"C:\\Users\\pv23228\\Documents\\P.AI\\Data\\Movies Dataset\\final_item_data_ss_trainset_v1.csv\"\n",
    "      }\n",
    "    }, \n",
    "      {\n",
    "      \"port\": 3,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"local\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\": r\"C:\\Users\\pv23228\\Documents\\P.AI\\Data\\Movies Dataset\\final_customer_demographics_ss_trainset_v1.csv\"\n",
    "      }\n",
    "    }    \n",
    "  ],\n",
    "  \"output\": [\n",
    "    {\n",
    "      \"port\": 5,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"s3\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\": \"s3://zs-ds-pzai-general/data/customer_embeddings_imdb_tr_full_v1.csv\"\n",
    "      }\n",
    "    },\n",
    "      {\n",
    "      \"port\": 6,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"s3\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\": \"s3://zs-ds-pzai-general/data/item_embeddings_imdb_tr_full_v1.csv\"\n",
    "      }\n",
    "    },\n",
    "      {\n",
    "      \"port\": 7,\n",
    "      \"dataType\": \"dataframe\",\n",
    "      \"sourceDetails\": {\n",
    "        \"source\": \"s3\",\n",
    "        \"fileFormat\": \"csv\",\n",
    "        \"filePath\": \"s3://zs-ds-pzai-general/data/link_embeddings_imdb_tr_full_v1.csv\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"function\": {\n",
    "    \"component\": \"objective\",\n",
    "    \"args\": {\n",
    "        \"features_for_item_node\":[],\n",
    "        \"features_for_customer_node\":[],\n",
    "        \"batch_size\": 2000,\n",
    "        \"epochs\": 4,\n",
    "        \"layer_size_of_hidden_layer\":[32],\n",
    "        \"number_of_neighbours_access\":[4]\n",
    "    }\n",
    "  },\n",
    "  \"meta\": {\n",
    "    \"triggeredBy\": \"Aditya Kothari\",\n",
    "    \"triggerTime\": \"2020-02-06 12:55:04\",\n",
    "    \"pipelineId\": \"pzai_pipeline_001\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# DO NOT  EDIT BELOW\n",
    "input_data = request_data[\"input\"]\n",
    "output_data = request_data[\"output\"]\n",
    "arguments = request_data[\"function\"][\"args\"]\n",
    "meta_data = request_data[\"meta\"]\n",
    "\n",
    "\n",
    "transactions = DataFrame().get(input_data,1)\n",
    "items = DataFrame().get(input_data,2)\n",
    "customers = DataFrame().get(input_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10363x15162 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 365170 stored elements in COOrdinate format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Graph(num_nodes={'users': 10363, 'items': 15162},\n",
      "      num_edges={('users', 'ratings', 'items'): 365170},\n",
      "      metagraph=[('users', 'items', 'ratings')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rating': tensor([3., 5., 3.,  ..., 2., 4., 2.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix=csv_to_sparse(transactions)\n",
    "display(matrix)\n",
    "print(matrix.todense())\n",
    "g = dgl.bipartite_from_scipy(matrix, utype='users', etype='ratings', vtype='items', eweight_name='rating')\n",
    "print(g)\n",
    "g.edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10363, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding a column to Customer Df as there are no features for users in IMDB\n",
    "customers[\"alive\"] = 1\n",
    "#coverting dtype for live column to category to experiment on building tensor for category type feature\n",
    "customers[\"alive\"] = customers[\"alive\"].astype('category')\n",
    "customers = customers.drop_duplicates()\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4799, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = items.drop_duplicates()\n",
    "items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing user and product ids as a list, creating dict\n",
    "user_ids = list(customers.customer_id)\n",
    "product_ids = list(items.item_id)\n",
    "user_ids_invmap = {id_: i for i, id_ in enumerate(user_ids)}\n",
    "product_ids_invmap = {id_: i for i, id_ in enumerate(product_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Graph\n",
    "g= dgl.DGLGraph(multigraph=True)\n",
    "g.add_nodes(len(user_ids) + len(product_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Current HeteroNodeDataView has multiple node types, please passing the node type and the corresponding data through a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5cadace97628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 0 for padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mudata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_column\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pai_dglv5_item\\lib\\site-packages\\dgl\\view.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ntype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;34m'Current HeteroNodeDataView has multiple node types, '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                 \u001b[1;34m'please passing the node type and the corresponding data through a dict.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Current HeteroNodeDataView has multiple node types, please passing the node type and the corresponding data through a dict."
     ]
    }
   ],
   "source": [
    "# user features\n",
    "user_column =customers.columns[1]\n",
    "udata = torch.zeros(g.number_of_nodes(), dtype=torch.int64)\n",
    "# 0 for padding\n",
    "udata[:len(user_ids)] = torch.LongTensor(customers[user_column].cat.codes.values.astype('int64') + 1)\n",
    "g.ndata[user_column] = udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['users']['alive'] = udata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITEM features\n",
    "for item_column in ['budget','product_category', 'popularity','runtime', 'vote_count']:\n",
    "    pdata = torch.from_numpy(items[item_column].values.astype('int64'))\n",
    "    g.ndata['items'][item_column] = pdata\n",
    "#     g.ndata[item_column] = torch.zeros(g.number_of_nodes(), dtype=torch.int64)\n",
    "#     g.ndata[item_column][len(user_ids):len(user_ids) + len(product_ids)] = pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item categorical features\n",
    "for col in ['status', 'adult']:\n",
    "    items[col] = items[col].astype('category')\n",
    "#     pdata = torch.zeros(g.number_of_nodes(), dtype=torch.int64)\n",
    "    pdata = torch.LongTensor(items[col].cat.codes.values.astype('int64') + 1)\n",
    "    g.ndata['items'][col] = pdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/4799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\pv23228\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\pv23228\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4799/4799 [02:03<00:00, 38.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 4799/4799 [00:00<00:00, 11152.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Movie title\n",
    "nlp = stanfordnlp.Pipeline(use_gpu=False, processors='tokenize,lemma')\n",
    "vocab = set()\n",
    "title_words = []\n",
    "for t in tqdm.tqdm(items['title'].values):\n",
    "    doc = nlp(t)\n",
    "    words = set()\n",
    "    for s in doc.sentences:\n",
    "        words.update(w.lemma.lower() for w in s.words\n",
    "                     if not re.fullmatch(r'['+string.punctuation+']+', w.lemma))\n",
    "    vocab.update(words)\n",
    "    title_words.append(words)\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocab_invmap = {w: i for i, w in enumerate(vocab)}\n",
    "# bag-of-words\n",
    "g.ndata['title'] = torch.zeros(g.number_of_nodes(), len(vocab))\n",
    "\n",
    "\n",
    "for i, tw in enumerate(tqdm.tqdm(title_words)):\n",
    "    g.ndata['title'][i, [vocab_invmap[w] for w in tw]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6216.0</td>\n",
       "      <td>i1487</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9733.0</td>\n",
       "      <td>i2132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9488.0</td>\n",
       "      <td>i26391</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4614.0</td>\n",
       "      <td>i39183</td>\n",
       "      <td>4.0</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c5980.0</td>\n",
       "      <td>i788</td>\n",
       "      <td>4.0</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id item_id  price  product_count\n",
       "0     c6216.0   i1487    3.0             63\n",
       "1     c9733.0   i2132    5.0            103\n",
       "2     c9488.0  i26391    3.0              2\n",
       "3     c4614.0  i39183    4.0            263\n",
       "4     c5980.0    i788    4.0            710"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings= transactions.drop(columns = [\"product_category\", \"date\"]).drop_duplicates()\n",
    "product_count = ratings['item_id'].value_counts()\n",
    "product_count.name = 'product_count'\n",
    "ratings = ratings.join(product_count, on='item_id')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_time = None\n",
    "from functools import partial\n",
    "\n",
    "def split_user(df, filter_counts=0, timestamp=None):\n",
    "    df_new = df.copy()\n",
    "    df_new['prob'] = -1\n",
    "    df_new_sub = (df_new['product_count'] >= filter_counts).to_numpy().nonzero()[0]\n",
    "    prob = np.linspace(0, 1, df_new_sub.shape[0], endpoint=False)\n",
    "    np.random.shuffle(prob)\n",
    "    df_new['prob'].iloc[df_new_sub] = prob\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def data_split(ratings):\n",
    "    ratings = ratings.groupby('customer_id', group_keys=False).apply(\n",
    "            partial(split_user, filter_counts=5, timestamp=split_by_time))\n",
    "    ratings['train'] = ratings['prob'] <= 0.8\n",
    "    ratings['valid'] = (ratings['prob'] > 0.8) & (ratings['prob'] <= 0.9)\n",
    "    ratings['test'] = ratings['prob'] > 0.9\n",
    "    ratings.drop(['prob'], axis=1, inplace=True)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_count</th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38553</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i4226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1303</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i54503</td>\n",
       "      <td>3.5</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32030</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2104</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i858</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1853</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4817</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i2959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1942</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id item_id  price  product_count  train  valid   test\n",
       "38553        c1.0   i4226    4.0           1303   True  False  False\n",
       "18957        c1.0  i54503    3.5            352   True  False  False\n",
       "32030        c1.0    i110    1.0           2104   True  False  False\n",
       "7086         c1.0    i858    5.0           1853   True  False  False\n",
       "4817         c1.0   i2959    4.0           1942   True  False  False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "ratings_ = data_split(ratings)\n",
    "ratings_.head()\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_count</th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i4226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1303</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i54503</td>\n",
       "      <td>3.5</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2104</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i858</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1853</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1.0</td>\n",
       "      <td>i2959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1942</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id item_id  price  product_count  train  valid   test\n",
       "0        c1.0   i4226    4.0           1303   True  False  False\n",
       "1        c1.0  i54503    3.5            352   True  False  False\n",
       "2        c1.0    i110    1.0           2104   True  False  False\n",
       "3        c1.0    i858    5.0           1853   True  False  False\n",
       "4        c1.0   i2959    4.0           1942   True  False  False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_.reset_index(drop = True, inplace = True)\n",
    "ratings_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use ratings df to get lists of source-dest nodes \n",
    "rating_user_vertices = [user_ids_invmap[id_] for id_ in ratings_['customer_id'].values]\n",
    "rating_product_vertices = [product_ids_invmap[id_] + len(user_ids)\n",
    "                         for id_ in ratings_['item_id'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Partial, edges added \n",
    "# g.add_edges(\n",
    "#         rating_user_vertices,\n",
    "#         rating_product_vertices,\n",
    "#         data={'inv': torch.ones(ratings_.shape[0], dtype=torch.uint8),\n",
    "#             'rating': torch.FloatTensor(ratings_['price'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask():\n",
    "        valid_tensor = torch.from_numpy(ratings_['valid'].values.astype('uint8'))\n",
    "        test_tensor = torch.from_numpy(ratings_['test'].values.astype('uint8'))\n",
    "        train_tensor = torch.from_numpy(ratings_['train'].values.astype('uint8'))\n",
    "        g.edata['valid'] = valid_tensor\n",
    "        g.edata['test'] = test_tensor\n",
    "        g.edata['true'] = train_tensor\n",
    "        \n",
    "# Generate the list of products for each user in training/validation/test set.\n",
    "def generate_candidates():\n",
    "    p_train = []\n",
    "    p_valid = []\n",
    "    p_test = []\n",
    "    for uid in tqdm.tqdm(user_ids):\n",
    "        user_ratings = ratings_[ratings_['customer_id'] == uid]\n",
    "        p_train.append(np.array(\n",
    "            [product_ids_invmap[i] for i in user_ratings[user_ratings['train']]['item_id'].values]))\n",
    "        p_valid.append(np.array(\n",
    "            [product_ids_invmap[i] for i in user_ratings[user_ratings['valid']]['item_id'].values]))\n",
    "        p_test.append(np.array(\n",
    "            [product_ids_invmap[i] for i in user_ratings[user_ratings['test']]['item_id'].values]))\n",
    "        \n",
    "    return p_train, p_valid, p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10363/10363 [10:02<00:00, 17.20it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_mask()\n",
    "\n",
    "p_train, p_valid, p_test = generate_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the subgraph of all \"training\" edges\n",
    "g_train = g.edge_subgraph(g.filter_edges(lambda edges: edges.data['train']), True)\n",
    "# g_train.copy_from_parent()\n",
    "# g_train.readonly()\n",
    "\n",
    "#Obtain edge id's for valid and test data\n",
    "eid_valid = g.filter_edges(lambda edges: edges.data['valid'])\n",
    "eid_test = g.filter_edges(lambda edges: edges.data['test'])\n",
    "\n",
    "#Storing source and dest node ids for train, valid, test edges\n",
    "src_valid, dst_valid = g.find_edges(eid_valid)\n",
    "src_test, dst_test = g.find_edges(eid_test)\n",
    "src, dst = g_train.all_edges()\n",
    "\n",
    "#storing ratings for train, valid, test edges\n",
    "rating = g_train.edata['rating']\n",
    "rating_valid = g.edges[eid_valid].data['rating']\n",
    "rating_test = g.edges[eid_test].data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define feature size and layers in gNN\n",
    "feature_size = 10\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.031308650970459 Validation RMSE: 3.42280650138855 Test RMSE: 3.4068078994750977\n",
      "Training loss: 6.358283519744873 Validation RMSE: 2.653252124786377 Test RMSE: 2.635495662689209\n",
      "Training loss: 5.915226459503174 Validation RMSE: 2.545581102371216 Test RMSE: 2.525508403778076\n",
      "Training loss: 5.876169681549072 Validation RMSE: 2.4451513290405273 Test RMSE: 2.4228813648223877\n",
      "Training loss: 5.133886814117432 Validation RMSE: 2.3491814136505127 Test RMSE: 2.3247969150543213\n",
      "Training loss: 4.962716579437256 Validation RMSE: 2.25700306892395 Test RMSE: 2.2305908203125\n",
      "Training loss: 4.498600959777832 Validation RMSE: 2.168396472930908 Test RMSE: 2.1400582790374756\n",
      "Training loss: 4.082102298736572 Validation RMSE: 2.0832841396331787 Test RMSE: 2.0531299114227295\n",
      "Training loss: 3.7747926712036133 Validation RMSE: 2.0018880367279053 Test RMSE: 1.970004677772522\n",
      "Training loss: 3.2147936820983887 Validation RMSE: 1.923883080482483 Test RMSE: 1.8903712034225464\n",
      "Training loss: 3.113598585128784 Validation RMSE: 1.8494857549667358 Test RMSE: 1.8144303560256958\n",
      "Training loss: 2.830751419067383 Validation RMSE: 1.7785667181015015 Test RMSE: 1.7421683073043823\n",
      "Training loss: 2.6401448249816895 Validation RMSE: 1.7111754417419434 Test RMSE: 1.6734817028045654\n",
      "Training loss: 2.489861249923706 Validation RMSE: 1.647313117980957 Test RMSE: 1.6085096597671509\n",
      "Training loss: 2.464437246322632 Validation RMSE: 1.586970567703247 Test RMSE: 1.5471887588500977\n",
      "Training loss: 2.214543581008911 Validation RMSE: 1.5300612449645996 Test RMSE: 1.4894146919250488\n",
      "Training loss: 1.9305254220962524 Validation RMSE: 1.4765528440475464 Test RMSE: 1.4351863861083984\n",
      "Training loss: 1.9668322801589966 Validation RMSE: 1.4265614748001099 Test RMSE: 1.3846126794815063\n",
      "Training loss: 1.8260737657546997 Validation RMSE: 1.3797982931137085 Test RMSE: 1.3374876976013184\n",
      "Training loss: 1.9052996635437012 Validation RMSE: 1.3363673686981201 Test RMSE: 1.293765664100647\n",
      "Training loss: 1.4765946865081787 Validation RMSE: 1.295987844467163 Test RMSE: 1.2532811164855957\n",
      "Training loss: 1.5424153804779053 Validation RMSE: 1.2587172985076904 Test RMSE: 1.216071605682373\n",
      "Training loss: 1.4097131490707397 Validation RMSE: 1.2243812084197998 Test RMSE: 1.1819199323654175\n",
      "Training loss: 1.2794166803359985 Validation RMSE: 1.1928807497024536 Test RMSE: 1.1507370471954346\n",
      "Training loss: 1.2549877166748047 Validation RMSE: 1.1639511585235596 Test RMSE: 1.1222560405731201\n",
      "Training loss: 1.2702096700668335 Validation RMSE: 1.1376439332962036 Test RMSE: 1.096537470817566\n",
      "Training loss: 1.2884013652801514 Validation RMSE: 1.1136016845703125 Test RMSE: 1.0731793642044067\n",
      "Training loss: 1.2592027187347412 Validation RMSE: 1.091856598854065 Test RMSE: 1.052183985710144\n",
      "Training loss: 1.1026843786239624 Validation RMSE: 1.072034239768982 Test RMSE: 1.0332696437835693\n",
      "Training loss: 1.2648565769195557 Validation RMSE: 1.054150938987732 Test RMSE: 1.0162813663482666\n",
      "Training loss: 1.1446316242218018 Validation RMSE: 1.038027286529541 Test RMSE: 1.0011427402496338\n",
      "Training loss: 1.1494555473327637 Validation RMSE: 1.0234246253967285 Test RMSE: 0.9875685572624207\n",
      "Training loss: 1.0025314092636108 Validation RMSE: 1.0102649927139282 Test RMSE: 0.97549968957901\n",
      "Training loss: 0.8962063193321228 Validation RMSE: 0.9983887076377869 Test RMSE: 0.9646828174591064\n",
      "Training loss: 1.0715067386627197 Validation RMSE: 0.9876577854156494 Test RMSE: 0.9550753235816956\n",
      "Training loss: 0.923481822013855 Validation RMSE: 0.9779393076896667 Test RMSE: 0.9465044140815735\n",
      "Training loss: 1.06045401096344 Validation RMSE: 0.9691736698150635 Test RMSE: 0.9388047456741333\n",
      "Training loss: 0.7728304862976074 Validation RMSE: 0.9611899852752686 Test RMSE: 0.9319729208946228\n",
      "Training loss: 0.9586930871009827 Validation RMSE: 0.9539682269096375 Test RMSE: 0.9258463978767395\n",
      "Training loss: 0.9792560338973999 Validation RMSE: 0.9474307298660278 Test RMSE: 0.9203954339027405\n",
      "Training loss: 0.7939832806587219 Validation RMSE: 0.9415263533592224 Test RMSE: 0.9154835939407349\n",
      "Training loss: 1.1159988641738892 Validation RMSE: 0.9360915422439575 Test RMSE: 0.9110461473464966\n",
      "Training loss: 0.945624589920044 Validation RMSE: 0.9311078190803528 Test RMSE: 0.9071013331413269\n",
      "Training loss: 1.0491890907287598 Validation RMSE: 0.9264818429946899 Test RMSE: 0.903437614440918\n",
      "Training loss: 0.8463597297668457 Validation RMSE: 0.9223215579986572 Test RMSE: 0.9001699686050415\n",
      "Training loss: 0.8386476635932922 Validation RMSE: 0.9184297919273376 Test RMSE: 0.8971638083457947\n",
      "Training loss: 0.8522619009017944 Validation RMSE: 0.9148209691047668 Test RMSE: 0.8944103121757507\n",
      "Training loss: 0.7369750142097473 Validation RMSE: 0.9114798307418823 Test RMSE: 0.891936182975769\n",
      "Training loss: 0.6693294644355774 Validation RMSE: 0.9084581732749939 Test RMSE: 0.8897174000740051\n",
      "Training loss: 0.9303247928619385 Validation RMSE: 0.9055598378181458 Test RMSE: 0.8876229524612427\n"
     ]
    }
   ],
   "source": [
    "#Build model \n",
    "model = GraphSAGERecommender(GraphSageWithSampling(100, 1, g_train))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-9)\n",
    "\n",
    "batch_size = 1024\n",
    "n_users = len(customers['customer_id'].to_list())\n",
    "n_products = len(items['item_id'].to_list())\n",
    "\n",
    "#Running model training for 50 epochs\n",
    "for epoch in range(50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Validation & Test, we precompute GraphSage output for all nodes first.\n",
    "    sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "        g_train,\n",
    "        batch_size,\n",
    "        5,\n",
    "        1,\n",
    "        seed_nodes=torch.arange(g.number_of_nodes()),\n",
    "        prefetch=True,\n",
    "        add_self_loop=True,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h = []\n",
    "        for nf in sampler:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            h.append(model.gcn.forward(nf))\n",
    "        h = torch.cat(h)\n",
    "\n",
    "        # Compute validation RMSE\n",
    "        score = torch.zeros(len(src_valid))\n",
    "        for i in range(0, len(src_valid), batch_size):\n",
    "            s = src_valid[i:i+batch_size]\n",
    "            d = dst_valid[i:i+batch_size]\n",
    "            score[i:i+batch_size] = (h[s] * h[d]).sum(1) + model.node_biases[s + 1] + model.node_biases[d + 1]\n",
    "        valid_rmse = ((score - rating_valid) ** 2).mean().sqrt()\n",
    "\n",
    "        # Compute test RMSE\n",
    "        score = torch.zeros(len(src_test))\n",
    "        for i in range(0, len(src_test), batch_size):\n",
    "            s = src_test[i:i+batch_size]\n",
    "            d = dst_test[i:i+batch_size]\n",
    "            score[i:i+batch_size] = (h[s] * h[d]).sum(1) + model.node_biases[s + 1] + model.node_biases[d + 1]\n",
    "        test_rmse = ((score - rating_test) ** 2).mean().sqrt()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    shuffle_idx = torch.randperm(g_train.number_of_edges())\n",
    "    src_shuffled = src[shuffle_idx]\n",
    "    dst_shuffled = dst[shuffle_idx]\n",
    "    rating_shuffled = rating[shuffle_idx]\n",
    "    src_batches = src_shuffled.split(batch_size)\n",
    "    dst_batches = dst_shuffled.split(batch_size)\n",
    "    rating_batches = rating_shuffled.split(batch_size)\n",
    "\n",
    "    seed_nodes = torch.cat(sum([[s, d] for s, d in zip(src_batches, dst_batches)], []))\n",
    "    \n",
    "    sampler = dgl.contrib.sampling.NeighborSampler(\n",
    "        g_train,               # the graph\n",
    "        batch_size * 2,        # number of nodes to compute at a time, HACK 2\n",
    "        5,                     # number of neighbors for each node\n",
    "        1,                     # number of layers in GCN\n",
    "        seed_nodes=seed_nodes, # list of seed nodes, HACK 2\n",
    "        prefetch=True,         # whether to prefetch the NodeFlows\n",
    "        add_self_loop=True,    # whether to add a self-loop in the NodeFlows, HACK 1\n",
    "        shuffle=False,         # whether to shuffle the seed nodes.  Should be False here.\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    for s, d, r, nodeflow in zip(src_batches, dst_batches, rating_batches, sampler):\n",
    "        score = model.forward(nodeflow, s, d)\n",
    "        loss = ((score - r) ** 2).mean()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('Training loss:', loss.item(), 'Validation RMSE:', valid_rmse.item(), 'Test RMSE:', test_rmse.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import dgl\n",
    ">>> import scipy as sp\n",
    ">>> m = sp.sparse.random(100, 100, density=0.1, format='csr')\n",
    ">>> g= dgl.DGLGraph(m, readonly=True)\n",
    ">>> g.number_of_nodes()\n",
    "100\n",
    ">>> g.number_of_edges()\n",
    "1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGERecommender(\n",
       "  (gcn): GraphSageWithSampling(\n",
       "    (convs): ModuleList(\n",
       "      (0): GraphSageConvWithSampling(\n",
       "        (W): Linear(in_features=20, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (emb): ModuleDict(\n",
       "      (alive): Embedding(2, 10, padding_idx=0)\n",
       "      (budget): Embedding(380000001, 10, padding_idx=0)\n",
       "      (product_category): Embedding(11, 10, padding_idx=0)\n",
       "      (popularity): Embedding(141, 10, padding_idx=0)\n",
       "      (runtime): Embedding(481, 10, padding_idx=0)\n",
       "      (vote_count): Embedding(12270, 10, padding_idx=0)\n",
       "      (status): Embedding(5, 10, padding_idx=0)\n",
       "      (adult): Embedding(3, 10, padding_idx=0)\n",
       "      (_ID): Embedding(15162, 10, padding_idx=0)\n",
       "    )\n",
       "    (proj): ModuleDict(\n",
       "      (title): Sequential(\n",
       "        (0): Linear(in_features=4854, out_features=10, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (node_emb): Embedding(15163, 10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_embeddings(ndata, emb, proj):\n",
    "    \"\"\"Adds external (categorical and numeric) features into node representation G.ndata['h']\"\"\"\n",
    "    extra_repr = []\n",
    "    for key, value in ndata.items():\n",
    "        if (value.dtype == torch.int64) and key in emb:\n",
    "            result = emb[key](value)\n",
    "            if result.dim() == 3:    # bag of words: the result would be a (n_nodes x seq_len x feature_size) tensor\n",
    "                result = result.mean(1)\n",
    "            extra_repr.append(result)\n",
    "        elif (value.dtype == torch.float32) and key in proj:\n",
    "            result = proj[key](value)\n",
    "            extra_repr.append(result)\n",
    "    ndata['h'] = ndata['h'] + torch.stack(extra_repr, 0).sum(0)\n",
    "    \n",
    "def init_weight(param, initializer, nonlinearity):\n",
    "    initializer = getattr(nn.init, initializer)\n",
    "    if nonlinearity is not None:\n",
    "        initializer(param)\n",
    "    else:\n",
    "        initializer(param, nn.init.calculate_gain(nonlinearity))\n",
    "        \n",
    "def init_bias(param):\n",
    "    nn.init.constant_(param, 0)\n",
    "\n",
    "class GraphSageConvWithSampling(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(GraphSageConvWithSampling, self).__init__()\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.W = nn.Linear(feature_size * 2, feature_size)\n",
    "        init_weight(self.W.weight, 'xavier_uniform_', 'leaky_relu')\n",
    "        init_bias(self.W.bias)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        h_agg = nodes.data['h_agg']\n",
    "        h = nodes.data['h']\n",
    "        w = nodes.data['w'][:, None]\n",
    "        h_agg = (h_agg-h)/(w-1).clamp(min=1)    # HACK 1\n",
    "        h_concat = torch.cat([h, h_agg], 1)\n",
    "        h_new = F.leaky_relu(self.W(h_concat))\n",
    "        return {'h': h_new / h_new.norm(dim=1, keepdim=True).clamp(min=1e-6)}\n",
    "    \n",
    "class GraphSageWithSampling(nn.Module):\n",
    "    def __init__(self, feature_size, n_layers, G):\n",
    "        super(GraphSageWithSampling, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.convs = nn.ModuleList([GraphSageConvWithSampling(feature_size) for _ in range(n_layers)])\n",
    "        \n",
    "        self.emb = nn.ModuleDict()\n",
    "        self.proj = nn.ModuleDict()\n",
    "\n",
    "        for key, scheme in G.node_attr_schemes().items():\n",
    "            if scheme.dtype == torch.int64:\n",
    "                n_items = G.ndata[key].max().item()\n",
    "                self.emb[key] = nn.Embedding(\n",
    "                        n_items + 1,\n",
    "                        self.feature_size,\n",
    "                        padding_idx=0)\n",
    "                nn.init.normal_(self.emb[key].weight, 1 / self.feature_size)\n",
    "            elif scheme.dtype == torch.float32:\n",
    "                w = nn.Linear(scheme.shape[0], self.feature_size)\n",
    "                init_weight(w.weight, 'xavier_uniform_', 'leaky_relu')\n",
    "                init_bias(w.bias)\n",
    "                self.proj[key] = nn.Sequential(w, nn.LeakyReLU())\n",
    "                \n",
    "        self.G = G\n",
    "        \n",
    "        self.node_emb = nn.Embedding(G.number_of_nodes() + 1, feature_size)\n",
    "        nn.init.normal_(self.node_emb.weight, std=1 / self.feature_size)\n",
    "\n",
    "    msg = [FN.copy_src('h', 'h'),\n",
    "           FN.copy_src('one', 'one')]\n",
    "    red = [FN.sum('h', 'h_agg'), FN.sum('one', 'w')]\n",
    "\n",
    "    def forward(self, nf):\n",
    "        '''\n",
    "        nf: NodeFlow.\n",
    "        '''\n",
    "        nf.copy_from_parent(edge_embed_names=None)\n",
    "        for i in range(nf.num_layers):\n",
    "            nf.layers[i].data['h'] = self.node_emb(nf.layer_parent_nid(i) + 1)\n",
    "            nf.layers[i].data['one'] = torch.ones(nf.layer_size(i))\n",
    "            mix_embeddings(nf.layers[i].data, model.gcn.emb, model.gcn.proj)\n",
    "        if self.n_layers == 0:\n",
    "            return nf.layers[i].data['h']\n",
    "        for i in range(self.n_layers):\n",
    "            nf.block_compute(i, self.msg, self.red, self.convs[i])\n",
    "\n",
    "        result = nf.layers[self.n_layers].data['h']\n",
    "        assert (result != result).sum() == 0\n",
    "        return result\n",
    "    \n",
    "class GraphSAGERecommender(nn.Module):\n",
    "    def __init__(self, gcn):\n",
    "        super(GraphSAGERecommender, self).__init__()\n",
    "        \n",
    "        self.gcn = gcn\n",
    "        self.node_biases = nn.Parameter(torch.zeros(gcn.G.number_of_nodes()+1))\n",
    "        \n",
    "    def forward(self, nf, src, dst):\n",
    "        h_output = self.gcn(nf)\n",
    "        h_src = h_output[nodeflow.map_from_parent_nid(-1, src, True)]\n",
    "        h_dst = h_output[nodeflow.map_from_parent_nid(-1, dst, True)]\n",
    "        score = (h_src * h_dst).sum(1) + self.node_biases[src+1] + self.node_biases[dst+1]\n",
    "        return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
